from llama_index.core import Document, Settings

import LLM
import utils
import ingestion

def main():
    # ==========================================================================
    # PASSO 1: CONFIGURA√á√ÉO GLOBAL
    # ==========================================================================
    # Define o modelo de Embedding que ser√° usado para vetorizar
    # O LlamaIndex usar√° isso automaticamente dentro do ingestion.py
    Settings.embed_model = LLM.embed_model
    
    # ==========================================================================
    # PASSO 2: DEFINIR FONTES
    # ==========================================================================
    urls_para_ler = [
        "http://www.planalto.gov.br/ccivil_03/leis/lcp/lcp123.htm",
        "https://www.planalto.gov.br/ccivil_03/decreto-lei/del5452.htm"
    ]
    
    lista_final_documentos = []

    print(f"üöÄ Iniciando processamento de {len(urls_para_ler)} leis...")

    # ==========================================================================
    # PASSO 3: LOOP DE PROCESSAMENTO (Extra√ß√£o -> Fatiamento -> Convers√£o)
    # ==========================================================================
    

    # ==========================================================================
    # PASSO 4: INGEST√ÉO (Salvar no Qdrant)
    # ==========================================================================
    print(f"\nüíæ Iniciando grava√ß√£o de {len(lista_final_documentos)} vetores no Qdrant...")
    
    # Chama a fun√ß√£o do arquivo ingestion.py
    ingestion.run_ingestion(lista_final_documentos)
    
    print("\n‚úÖ Processo Finalizado! Os dados est√£o indexados.")

if __name__ == "__main__":
    main()